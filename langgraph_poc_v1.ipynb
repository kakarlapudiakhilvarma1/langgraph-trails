{"cells":[{"cell_type":"markdown","metadata":{"id":"RZNj8qWtslYp"},"source":["#### **LangGraph - directory loader, history aware, streaming, faiss-index**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":24869,"status":"ok","timestamp":1762055772033,"user":{"displayName":"Akhil Kakarlapudi","userId":"04814246923552821671"},"user_tz":-330},"id":"pP6b71K5r66d","outputId":"169d0d5f-69f9-4c44-d486-d7798e1760da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting langchain_community\n","  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n","Collecting langchain_huggingface\n","  Downloading langchain_huggingface-1.0.0-py3-none-any.whl.metadata (2.1 kB)\n","Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.79)\n","Collecting langgraph\n","  Downloading langgraph-1.0.2-py3-none-any.whl.metadata (7.4 kB)\n","Collecting langchain_google_genai\n","  Downloading langchain_google_genai-3.0.0-py3-none-any.whl.metadata (7.1 kB)\n","Collecting pypdf\n","  Downloading pypdf-6.1.3-py3-none-any.whl.metadata (7.1 kB)\n","Collecting faiss-cpu\n","  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n","Collecting langchain-core\n","  Downloading langchain_core-1.0.2-py3-none-any.whl.metadata (3.5 kB)\n","Collecting langchain-classic\u003c2.0.0,\u003e=1.0.0 (from langchain_community)\n","  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n","Requirement already satisfied: SQLAlchemy\u003c3.0.0,\u003e=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n","Collecting requests\u003c3.0.0,\u003e=2.32.5 (from langchain_community)\n","  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: PyYAML\u003c7.0.0,\u003e=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n","Requirement already satisfied: aiohttp\u003c4.0.0,\u003e=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.1)\n","Requirement already satisfied: tenacity!=8.4.0,\u003c10.0.0,\u003e=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n","Collecting dataclasses-json\u003c0.7.0,\u003e=0.6.7 (from langchain_community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Requirement already satisfied: pydantic-settings\u003c3.0.0,\u003e=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n","Requirement already satisfied: langsmith\u003c1.0.0,\u003e=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.38)\n","Requirement already satisfied: httpx-sse\u003c1.0.0,\u003e=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n","Requirement already satisfied: numpy\u003e=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n","INFO: pip is looking at multiple versions of langchain to determine which version is compatible with other requirements. This could take a while.\n","Collecting langchain\n","  Downloading langchain-1.0.3-py3-none-any.whl.metadata (4.7 kB)\n","Requirement already satisfied: pydantic\u003c3.0.0,\u003e=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n","Requirement already satisfied: huggingface-hub\u003c1.0.0,\u003e=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.36.0)\n","Requirement already satisfied: tokenizers\u003c1.0.0,\u003e=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.1)\n","Requirement already satisfied: jsonpatch\u003c2.0.0,\u003e=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n","Requirement already satisfied: packaging\u003c26.0.0,\u003e=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n","Requirement already satisfied: typing-extensions\u003c5.0.0,\u003e=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n","Collecting langgraph-checkpoint\u003c4.0.0,\u003e=2.1.0 (from langgraph)\n","  Downloading langgraph_checkpoint-3.0.0-py3-none-any.whl.metadata (4.2 kB)\n","Collecting langgraph-prebuilt\u003c1.1.0,\u003e=1.0.2 (from langgraph)\n","  Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\n","Collecting langgraph-sdk\u003c0.3.0,\u003e=0.2.2 (from langgraph)\n","  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: xxhash\u003e=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n","Collecting google-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0 (from langchain_google_genai)\n","  Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl.metadata (10 kB)\n","Collecting filetype\u003c2.0.0,\u003e=1.2.0 (from langchain_google_genai)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: aiohappyeyeballs\u003e=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain_community) (2.6.1)\n","Requirement already satisfied: aiosignal\u003e=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain_community) (1.4.0)\n","Requirement already satisfied: attrs\u003e=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain_community) (25.4.0)\n","Requirement already satisfied: frozenlist\u003e=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain_community) (1.8.0)\n","Requirement already satisfied: multidict\u003c7.0,\u003e=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain_community) (6.7.0)\n","Requirement already satisfied: propcache\u003e=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain_community) (0.4.1)\n","Requirement already satisfied: yarl\u003c2.0,\u003e=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp\u003c4.0.0,\u003e=3.8.3-\u003elangchain_community) (1.22.0)\n","Collecting marshmallow\u003c4.0.0,\u003e=3.18.0 (from dataclasses-json\u003c0.7.0,\u003e=0.6.7-\u003elangchain_community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect\u003c1,\u003e=0.4.0 (from dataclasses-json\u003c0.7.0,\u003e=0.6.7-\u003elangchain_community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,\u003c3.0.0,\u003e=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,\u003c3.0.0,\u003e=1.34.1-\u003egoogle-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (2.28.0)\n","Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,\u003c3.0.0,\u003e=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (2.38.0)\n","Requirement already satisfied: grpcio\u003c2.0.0,\u003e=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (1.76.0)\n","Requirement already satisfied: proto-plus\u003c2.0.0,\u003e=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (1.26.1)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,\u003c7.0.0,\u003e=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (5.29.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.33.4-\u003elangchain_huggingface) (3.20.0)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.33.4-\u003elangchain_huggingface) (2025.3.0)\n","Requirement already satisfied: tqdm\u003e=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.33.4-\u003elangchain_huggingface) (4.67.1)\n","Requirement already satisfied: hf-xet\u003c2.0.0,\u003e=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub\u003c1.0.0,\u003e=0.33.4-\u003elangchain_huggingface) (1.2.0)\n","Requirement already satisfied: jsonpointer\u003e=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch\u003c2.0.0,\u003e=1.33.0-\u003elangchain-core) (3.0.0)\n","Collecting langchain-text-splitters\u003c2.0.0,\u003e=1.0.0 (from langchain-classic\u003c2.0.0,\u003e=1.0.0-\u003elangchain_community)\n","  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n","Collecting ormsgpack\u003e=1.10.0 (from langgraph-checkpoint\u003c4.0.0,\u003e=2.1.0-\u003elanggraph)\n","  Downloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: httpx\u003e=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk\u003c0.3.0,\u003e=0.2.2-\u003elanggraph) (0.28.1)\n","Requirement already satisfied: orjson\u003e=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk\u003c0.3.0,\u003e=0.2.2-\u003elanggraph) (3.11.4)\n","Requirement already satisfied: requests-toolbelt\u003e=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith\u003c1.0.0,\u003e=0.1.125-\u003elangchain_community) (1.0.0)\n","Requirement already satisfied: zstandard\u003e=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith\u003c1.0.0,\u003e=0.1.125-\u003elangchain_community) (0.25.0)\n","Requirement already satisfied: annotated-types\u003e=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic\u003c3.0.0,\u003e=2.7.4-\u003elangchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic\u003c3.0.0,\u003e=2.7.4-\u003elangchain) (2.33.2)\n","Requirement already satisfied: typing-inspection\u003e=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic\u003c3.0.0,\u003e=2.7.4-\u003elangchain) (0.4.2)\n","Requirement already satisfied: python-dotenv\u003e=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings\u003c3.0.0,\u003e=2.10.1-\u003elangchain_community) (1.2.1)\n","Requirement already satisfied: charset_normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.12/dist-packages (from requests\u003c3.0.0,\u003e=2.32.5-\u003elangchain_community) (3.4.4)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.12/dist-packages (from requests\u003c3.0.0,\u003e=2.32.5-\u003elangchain_community) (3.11)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests\u003c3.0.0,\u003e=2.32.5-\u003elangchain_community) (2.5.0)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests\u003c3.0.0,\u003e=2.32.5-\u003elangchain_community) (2025.10.5)\n","Requirement already satisfied: greenlet\u003e=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy\u003c3.0.0,\u003e=1.4.0-\u003elangchain_community) (3.2.4)\n","Requirement already satisfied: googleapis-common-protos\u003c2.0.0,\u003e=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,\u003c3.0.0,\u003e=1.34.1-\u003egoogle-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,\u003c3.0.0,\u003e=1.34.1-\u003egoogle-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (1.71.0)\n","Requirement already satisfied: grpcio-status\u003c2.0.0,\u003e=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,\u003c3.0.0,\u003e=1.34.1-\u003egoogle-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (1.71.2)\n","Requirement already satisfied: cachetools\u003c6.0,\u003e=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,\u003c3.0.0,\u003e=2.14.1-\u003egoogle-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (5.5.2)\n","Requirement already satisfied: pyasn1-modules\u003e=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,\u003c3.0.0,\u003e=2.14.1-\u003egoogle-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (0.4.2)\n","Requirement already satisfied: rsa\u003c5,\u003e=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,\u003c3.0.0,\u003e=2.14.1-\u003egoogle-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (4.9.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx\u003e=0.25.2-\u003elanggraph-sdk\u003c0.3.0,\u003e=0.2.2-\u003elanggraph) (4.11.0)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx\u003e=0.25.2-\u003elanggraph-sdk\u003c0.3.0,\u003e=0.2.2-\u003elanggraph) (1.0.9)\n","Requirement already satisfied: h11\u003e=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*-\u003ehttpx\u003e=0.25.2-\u003elanggraph-sdk\u003c0.3.0,\u003e=0.2.2-\u003elanggraph) (0.16.0)\n","Collecting mypy-extensions\u003e=0.3.0 (from typing-inspect\u003c1,\u003e=0.4.0-\u003edataclasses-json\u003c0.7.0,\u003e=0.6.7-\u003elangchain_community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: pyasn1\u003c0.7.0,\u003e=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules\u003e=0.2.1-\u003egoogle-auth!=2.24.0,!=2.25.0,\u003c3.0.0,\u003e=2.14.1-\u003egoogle-ai-generativelanguage\u003c1.0.0,\u003e=0.7.0-\u003elangchain_google_genai) (0.6.1)\n","Requirement already satisfied: sniffio\u003e=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio-\u003ehttpx\u003e=0.25.2-\u003elanggraph-sdk\u003c0.3.0,\u003e=0.2.2-\u003elanggraph) (1.3.1)\n","Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain-1.0.3-py3-none-any.whl (91 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_huggingface-1.0.0-py3-none-any.whl (27 kB)\n","Downloading langchain_core-1.0.2-py3-none-any.whl (469 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.3/469.3 kB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph-1.0.2-py3-none-any.whl (156 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_google_genai-3.0.0-py3-none-any.whl (57 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-6.1.3-py3-none-any.whl (323 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Downloading google_ai_generativelanguage-0.9.0-py3-none-any.whl (1.4 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_checkpoint-3.0.0-py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langgraph_prebuilt-1.0.2-py3-none-any.whl (34 kB)\n","Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading requests-2.32.5-py3-none-any.whl (64 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n","Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ormsgpack-1.11.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: filetype, requests, pypdf, ormsgpack, mypy-extensions, marshmallow, faiss-cpu, typing-inspect, langgraph-sdk, dataclasses-json, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain_huggingface, google-ai-generativelanguage, langgraph-prebuilt, langchain_google_genai, langchain-classic, langgraph, langchain_community, langchain\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.32.4\n","    Uninstalling requests-2.32.4:\n","      Successfully uninstalled requests-2.32.4\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.79\n","    Uninstalling langchain-core-0.3.79:\n","      Successfully uninstalled langchain-core-0.3.79\n","  Attempting uninstall: langchain-text-splitters\n","    Found existing installation: langchain-text-splitters 0.3.11\n","    Uninstalling langchain-text-splitters-0.3.11:\n","      Successfully uninstalled langchain-text-splitters-0.3.11\n","  Attempting uninstall: google-ai-generativelanguage\n","    Found existing installation: google-ai-generativelanguage 0.6.15\n","    Uninstalling google-ai-generativelanguage-0.6.15:\n","      Successfully uninstalled google-ai-generativelanguage-0.6.15\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.27\n","    Uninstalling langchain-0.3.27:\n","      Successfully uninstalled langchain-0.3.27\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n","google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed dataclasses-json-0.6.7 faiss-cpu-1.12.0 filetype-1.2.0 google-ai-generativelanguage-0.9.0 langchain-1.0.3 langchain-classic-1.0.0 langchain-core-1.0.2 langchain-text-splitters-1.0.0 langchain_community-0.4.1 langchain_google_genai-3.0.0 langchain_huggingface-1.0.0 langgraph-1.0.2 langgraph-checkpoint-3.0.0 langgraph-prebuilt-1.0.2 langgraph-sdk-0.2.9 marshmallow-3.26.1 mypy-extensions-1.1.0 ormsgpack-1.11.0 pypdf-6.1.3 requests-2.32.5 typing-inspect-0.9.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"f9b14b5df2474839b86f5bfb0811c8fb","pip_warning":{"packages":["google"]}}},"metadata":{},"output_type":"display_data"}],"source":["pip install langchain_community langchain langchain_huggingface langchain-core langgraph langchain_google_genai pypdf faiss-cpu"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3814,"status":"ok","timestamp":1762055840511,"user":{"displayName":"Akhil Kakarlapudi","userId":"04814246923552821671"},"user_tz":-330},"id":"VKlGTZQqtM_w"},"outputs":[],"source":["from google.colab import userdata\n","api_key = userdata.get('GOOGLE_API_KEY')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1796,"status":"ok","timestamp":1762055926473,"user":{"displayName":"Akhil Kakarlapudi","userId":"04814246923552821671"},"user_tz":-330},"id":"vqt8yyHFtTsH"},"outputs":[],"source":["import os\n","from typing import TypedDict, List, Literal\n","from langchain_core.messages import HumanMessage, AIMessage\n","from langgraph.graph import StateGraph, START, END\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_community.document_loaders import PyPDFDirectoryLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain_community.vectorstores import FAISS\n","from langchain_core.documents import Document\n","\n","\n","# Configuration\n","DEFAULT_PDF_DIRECTORY = \"/content/data\"\n","FAISS_INDEX_PATH = \"./faiss_index\"\n","\n","\n","# Enhanced State Definition\n","class AgentState(TypedDict):\n","    messages: list\n","    mode: str\n","    pdf_directory: str\n","    response: str\n","    source_documents: List[Document]\n","\n","\n","# Initialize LLM with streaming enabled\n","llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-2.5-pro\",\n","    temperature=0.3,\n","    api_key=api_key,\n","    #max_output_tokens= 1000,\n","    streaming=True\n",")\n","\n","\n","# Initialize embeddings globally\n","embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","\n","\n","# Node 1: Direct LLM\n","def direct_llm_node(state: AgentState):\n","    \"\"\"Direct interaction with LLM with conversation history\"\"\"\n","    messages = state[\"messages\"]\n","    response = llm.invoke(messages)\n","\n","    return {\n","        \"response\": response.content,\n","        \"messages\": messages + [AIMessage(content=response.content)],\n","        \"source_documents\": []\n","    }\n","\n","\n","# Node 2: RAG Pipeline with conversation history\n","def rag_pipeline_node(state: AgentState):\n","    \"\"\"Complete RAG pipeline with conversation history support\"\"\"\n","    messages = state[\"messages\"]\n","    current_query = messages[-1].content\n","\n","    pdf_directory = state.get(\"pdf_directory\", DEFAULT_PDF_DIRECTORY) or DEFAULT_PDF_DIRECTORY\n","\n","    # Load or create FAISS index\n","    if os.path.exists(FAISS_INDEX_PATH):\n","        print(\"Loading existing FAISS index...\")\n","        vectorstore = FAISS.load_local(\n","            FAISS_INDEX_PATH,\n","            embeddings,\n","            allow_dangerous_deserialization=True\n","        )\n","    else:\n","        print(f\"Creating new FAISS index from directory: {pdf_directory}\")\n","\n","        # Load all PDFs from directory\n","        loader = PyPDFDirectoryLoader(\n","            path=pdf_directory,\n","            glob=\"**/*.pdf\",  # Load all PDFs recursively\n","            recursive=True,    # Search subdirectories\n","            silent_errors=False  # Show errors if any\n","        )\n","\n","        documents = loader.load()\n","        print(f\"âœ“ Loaded {len(documents)} pages from PDF files in {pdf_directory}\")\n","\n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=500,\n","            chunk_overlap=150\n","        )\n","        chunks = text_splitter.split_documents(documents)\n","        print(f\"âœ“ Created {len(chunks)} chunks from documents\")\n","\n","        vectorstore = FAISS.from_documents(chunks, embeddings)\n","        vectorstore.save_local(FAISS_INDEX_PATH)\n","        print(f\"âœ“ FAISS index saved to {FAISS_INDEX_PATH}\")\n","\n","    # Retrieval\n","    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n","    relevant_docs = retriever.invoke(current_query)\n","    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n","\n","    # Build conversation history for context\n","    conversation_history = []\n","    for msg in messages[:-1]:  # Exclude current query\n","        if isinstance(msg, HumanMessage):\n","            conversation_history.append(f\"User: {msg.content}\")\n","        elif isinstance(msg, AIMessage):\n","            conversation_history.append(f\"Assistant: {msg.content}\")\n","\n","    history_text = \"\\n\".join(conversation_history) if conversation_history else \"No previous conversation.\"\n","\n","    # Enhanced prompt with conversation history\n","    prompt = f\"\"\"Based on the conversation history and context below, answer the current question.\n","\n","Conversation History:\n","{history_text}\n","\n","Context from documents:\n","{context}\n","\n","Current Question: {current_query}\n","\n","Answer (be specific and helpful):\"\"\"\n","\n","    response = llm.invoke(prompt)\n","\n","    return {\n","        \"response\": response.content,\n","        \"messages\": messages + [AIMessage(content=response.content)],\n","        \"source_documents\": relevant_docs\n","    }\n","\n","\n","# Routing Function\n","def route_query(state: AgentState) -\u003e Literal[\"direct_llm\", \"rag_pipeline\"]:\n","    \"\"\"Route based on mode\"\"\"\n","    if state[\"mode\"] == \"rag\":\n","        return \"rag_pipeline\"\n","    return \"direct_llm\"\n","\n","\n","# Build Graph\n","workflow = StateGraph(AgentState)\n","workflow.add_node(\"direct_llm\", direct_llm_node)\n","workflow.add_node(\"rag_pipeline\", rag_pipeline_node)\n","\n","workflow.add_conditional_edges(\n","    START,\n","    route_query,\n","    {\n","        \"direct_llm\": \"direct_llm\",\n","        \"rag_pipeline\": \"rag_pipeline\"\n","    }\n",")\n","\n","workflow.add_edge(\"direct_llm\", END)\n","workflow.add_edge(\"rag_pipeline\", END)\n","\n","# Compile\n","app = workflow.compile()\n","\n","\n","#  CHAT FUNCTION\n","conversation_history = []\n","\n","\n","def chat(query: str, mode: str = \"rag\", pdf_directory: str = \"\"):\n","    \"\"\"\n","    Chat function that maintains conversation history\n","\n","    Args:\n","        query: User's question\n","        mode: \"rag\" or \"direct\"\n","        pdf_directory: Path to directory containing PDFs (empty for default)\n","\n","    Returns:\n","        dict with response and source_documents\n","    \"\"\"\n","    # Add user message to history\n","    conversation_history.append(HumanMessage(content=query))\n","\n","    # Invoke with full conversation history\n","    result = app.invoke({\n","        \"messages\": conversation_history,\n","        \"mode\": mode,\n","        \"pdf_directory\": pdf_directory,\n","        \"response\": \"\",\n","        \"source_documents\": []\n","    })\n","\n","    # Add AI response to history\n","    conversation_history.append(AIMessage(content=result[\"response\"]))\n","\n","    return result\n","\n","\n","def reset_conversation():\n","    \"\"\"Clear conversation history to start fresh\"\"\"\n","    global conversation_history\n","    conversation_history = []\n","    print(\"ğŸ”„ Conversation history cleared!\")\n","\n","\n","def reset_faiss_index():\n","    \"\"\"Delete FAISS index to force recreation from PDFs\"\"\"\n","    import shutil\n","    if os.path.exists(FAISS_INDEX_PATH):\n","        shutil.rmtree(FAISS_INDEX_PATH)\n","        print(f\"ğŸ—‘ï¸ FAISS index deleted from {FAISS_INDEX_PATH}\")\n","    else:\n","        print(\"âš ï¸ No FAISS index found to delete\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"34LfcVSKttx4"},"outputs":[{"name":"stdout","output_type":"stream","text":["==================================================\n","Example 1: Direct LLM Mode\n","==================================================\n","\n","ğŸ¤– Response: Of course! Here is a comprehensive explanation of Generative AI, broken down for easy understanding.\n","\n","### The Simple Analogy: The Creative Apprentice\n","\n","Imagine you have an apprentice who wants to learn to paint. You don't give them a set of rules like \"a tree has a brown trunk and green leaves.\" Instead, you show them **millions of paintings** from every artist and every era.\n","\n","The apprentice studies all of themâ€”the brushstrokes of Van Gogh, the realism of Rembrandt, the abstracts of Picasso. After seeing all this art, they don't just copy one painting. They learn the underlying **patterns, styles, and relationships** that make up \"art.\"\n","\n","Now, you can ask the apprentice to create something new: \"Paint me a picture of an astronaut riding a horse in the style of Van Gogh.\"\n","\n","The apprentice, using everything it has learned, generates a **brand new, original painting** that fits your request. It's not a copy; it's a new creation based on a deep understanding of the data it was trained on.\n","\n","**Generative AI is that creative apprentice.**\n","\n","---\n","\n","### The Core Definition\n","\n","**Generative AI** is a type of artificial intelligence that can create new, original contentâ€”such as text, images, music, code, and videosâ€”that has not existed before.\n","\n","Unlike traditional AI, which is designed to recognize patterns and make predictions (e.g., classifying an email as spam), Generative AI is designed to **produce** or **generate** something new.\n","\n","### What Can Generative AI Create? (Examples)\n","\n","Generative AI is behind many of the amazing AI tools you see today:\n","\n","*   **Text:** It can write essays, emails, poems, and even computer code.\n","    *   **Examples:** **ChatGPT**, Google Bard/Gemini, Jasper AI.\n","*   **Images:** It can create realistic photos, digital art, and logos from a simple text description (a \"prompt\").\n","    *   **Examples:** **DALL-E 3**, **Midjourney**, Stable Diffusion.\n","*   **Code:** It can write, debug, and explain code in various programming languages.\n","    *   **Examples:** **GitHub Copilot**, Amazon CodeWhisperer.\n","*   **Audio \u0026 Music:** It can compose original musical pieces, generate realistic voiceovers, or create sound effects.\n","    *   **Examples:** Suno AI, ElevenLabs.\n","*   **Video:** It can generate video clips from text prompts or images, though this technology is still emerging.\n","    *   **Examples:** Sora (by OpenAI), RunwayML.\n","\n","### How Does It Work? (A Simplified Look)\n","\n","The process can be broken down into three main steps:\n","\n","1.  **Training:** The AI is trained on a massive dataset. For a text model like ChatGPT, this means feeding it a huge portion of the internetâ€”books, articles, websites, and conversations. For an image model like DALL-E, it's millions of images and their text descriptions. During this phase, the AI learns the patterns, grammar, structures, and relationships within the data.\n","\n","2.  **The Model:** The result of the training is a complex mathematical model, often a **neural network** (like a **Transformer**). This model is like the AI's \"brain.\" It doesn't store the data it was trained on, but rather a compressed, abstract understanding of it.\n","\n","3.  **Generation (Using a Prompt):** When you give the AI a prompt (e.g., \"Write a short story about a friendly robot\"), it uses its model to predict the most likely sequence of words (or pixels for an image) to create a response that fits your request. It generates the content word by word (or piece by piece), with each new piece being influenced by the ones that came before it.\n","\n","### The Key Difference: Generative vs. Traditional AI\n","\n","This is a crucial distinction:\n","\n","| Feature | **Traditional (Discriminative) AI** | **Generative AI** |\n","| :--- | :--- | :--- |\n","| **Goal** | To classify or predict from existing data. | To create new, original data. |\n","| **Question it Answers** | \"Is this a cat or a dog?\" | \"Create a picture of a cat wearing a hat.\" |\n","| **Output** | A label, a category, or a number (e.g., \"Spam,\" \"Cat,\" 95%). | New content (an image, a paragraph, a song). |\n","| **Example** | A facial recognition system that identifies a person. | An AI that creates a new, non-existent face. |\n","\n","### Why is Generative AI a Big Deal?\n","\n","Generative AI is considered a transformative technology for several reasons:\n","\n","*   **Democratizing Creativity:** It gives everyone the power to create high-quality content without needing years of specialized training.\n","*   **Boosting Productivity:** It can act as a \"co-pilot\" for writers, programmers, and designers, automating tedious tasks and helping to brainstorm ideas.\n","*   **New Possibilities:** It's unlocking new frontiers in fields like drug discovery (by generating new molecular structures) and entertainment (by creating new forms of interactive media).\n","\n","### Challenges and Concerns\n","\n","It's not without its problems:\n","\n","*   **Accuracy and \"Hallucinations\":** The AI can confidently make up facts and present them as truth.\n","*   **Bias:** If the training data contains biases (related to race, gender, etc.), the AI will learn and reproduce them.\n","*   **Ethical Issues:** Concerns about misinformation (deepfakes), copyright infringement, job displacement, and academic integrity are significant.\n","*   **Environmental Cost:** Training these massive models requires enormous amounts of computational power and energy.\n","\n","In short, **Generative AI is a powerful tool that learns from the vast library of human knowledge to create something entirely new.** It's a major leap forward in artificial intelligence, shifting the focus from simply understanding the world to actively contributing to it.\n","\n"]}],"source":["# Example 1 - Direct LLM\n","print(\"=\" * 50)\n","print(\"Example 1: Direct LLM Mode\")\n","print(\"=\" * 50)\n","\n","result0 = chat(\"What is Generative AI\", mode=\"direct\")\n","\n","print(\"\\nğŸ¤– Response: \", end=\"\")\n","for char in result0[\"response\"]:\n","    print(char, end=\"\", flush=True)\n","print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yzT0NH4wtumL"},"outputs":[],"source":["# Example 2 - Direct LLM - follow-up question\n","print(\"=\" * 50)\n","print(\"Example 2: Direct LLM Follow-up\")\n","print(\"=\" * 50)\n","\n","result00 = chat(\"How it was different from traditional machine learning?\", mode=\"direct\")\n","\n","print(\"\\nğŸ¤– Response: \", end=\"\")\n","for char in result00[\"response\"]:\n","    print(char, end=\"\", flush=True)\n","print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QXneUNx7tzUd"},"outputs":[],"source":["# Example 3: First RAG query\n","print(\"=\" * 50)\n","print(\"Example 3: RAG Mode (Multiple PDFs)\")\n","print(\"=\" * 50)\n","\n","result1 = chat(\"What is Beam Search?\", mode=\"rag\")\n","\n","print(\"\\nğŸ¤– Response: \", end=\"\")\n","for char in result1[\"response\"]:\n","    print(char, end=\"\", flush=True)\n","\n","print(\"\\n\\n--- Source Documents ---\")\n","for idx, doc in enumerate(result1[\"source_documents\"], 1):\n","    print(f\"\\nğŸ“„ Source {idx}:\")\n","    print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n","    print(f\"   File: {doc.metadata.get('source', 'N/A')}\")\n","    print(f\"   Content: {doc.page_content[:250]}...\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pzrf1K6gt1zf"},"outputs":[],"source":["# Example 4: Follow-up question (uses conversation history!)\n","print(\"\\n\" + \"=\" * 50)\n","print(\"Example 4: Follow-up Question\")\n","print(\"=\" * 50)\n","\n","result2 = chat(\"How it is different from other models?\", mode=\"rag\")\n","\n","print(\"\\nğŸ¤– Response: \", end=\"\")\n","for char in result2[\"response\"]:\n","    print(char, end=\"\", flush=True)\n","print(\"\\n\")\n","\n","print(\"\\n--- Source Documents ---\")\n","for idx, doc in enumerate(result2[\"source_documents\"], 1):\n","    print(f\"\\nğŸ“„ Source {idx}:\")\n","    print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n","    print(f\"   File: {doc.metadata.get('source', 'N/A')}\")\n","    print(f\"   Content: {doc.page_content[:250]}...\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNTyr3kAO7fY6s8SA7ORmTm","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}