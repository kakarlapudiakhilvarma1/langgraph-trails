{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZNj8qWtslYp"
      },
      "source": [
        "#### **Agentic RAG application using LangGraph**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP6b71K5r66d",
        "outputId": "e7e51a13-5fe9-49d1-ebcc-7988d5098b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: langchain_huggingface in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.12/dist-packages (3.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.3)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.38)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (0.9.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (2.28.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.11.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_community langchain langchain_huggingface langchain-core langgraph langchain_google_genai pypdf faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKlGTZQqtM_w"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "vqt8yyHFtTsH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import TypedDict, List, Literal\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "# Configuration\n",
        "DEFAULT_PDF_DIRECTORY = \"/content/data\"\n",
        "FAISS_INDEX_PATH = \"./faiss_index\"\n",
        "\n",
        "\n",
        "# Enhanced State Definition\n",
        "class AgentState(TypedDict):\n",
        "    messages: list\n",
        "    mode: str\n",
        "    pdf_directory: str\n",
        "    response: str\n",
        "    source_documents: List[Document]\n",
        "\n",
        "\n",
        "# Initialize LLM with streaming enabled\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=0.3,\n",
        "    api_key=api_key,\n",
        "    #max_output_tokens= 1000,\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize embeddings globally\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "# Node 1: Direct LLM\n",
        "def direct_llm_node(state: AgentState):\n",
        "    \"\"\"Direct interaction with LLM with conversation history\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    return {\n",
        "        \"response\": response.content,\n",
        "        \"messages\": messages + [AIMessage(content=response.content)],\n",
        "        \"source_documents\": []\n",
        "    }\n",
        "\n",
        "\n",
        "# Node 2: RAG Pipeline with conversation history\n",
        "def rag_pipeline_node(state: AgentState):\n",
        "    \"\"\"Complete RAG pipeline with conversation history support\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    current_query = messages[-1].content\n",
        "\n",
        "    pdf_directory = state.get(\"pdf_directory\", DEFAULT_PDF_DIRECTORY) or DEFAULT_PDF_DIRECTORY\n",
        "\n",
        "    # Load or create FAISS index\n",
        "    if os.path.exists(FAISS_INDEX_PATH):\n",
        "        print(\"Loading existing FAISS index...\")\n",
        "        vectorstore = FAISS.load_local(\n",
        "            FAISS_INDEX_PATH,\n",
        "            embeddings,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Creating new FAISS index from directory: {pdf_directory}\")\n",
        "\n",
        "        # Load all PDFs from directory\n",
        "        loader = PyPDFDirectoryLoader(\n",
        "            path=pdf_directory,\n",
        "            glob=\"**/*.pdf\",  # Load all PDFs recursively\n",
        "            recursive=True,    # Search subdirectories\n",
        "            silent_errors=False  # Show errors if any\n",
        "        )\n",
        "\n",
        "        documents = loader.load()\n",
        "        print(f\" Loaded {len(documents)} pages from PDF files in {pdf_directory}\")\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=150\n",
        "        )\n",
        "        chunks = text_splitter.split_documents(documents)\n",
        "        print(f\" Created {len(chunks)} chunks from documents\")\n",
        "\n",
        "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "        vectorstore.save_local(FAISS_INDEX_PATH)\n",
        "        print(f\" FAISS index saved to {FAISS_INDEX_PATH}\")\n",
        "\n",
        "    # Retrieval\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    relevant_docs = retriever.invoke(current_query)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "    # Build conversation history for context\n",
        "    conversation_history = []\n",
        "    for msg in messages[:-1]:  # Exclude current query\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            conversation_history.append(f\"User: {msg.content}\")\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            conversation_history.append(f\"Assistant: {msg.content}\")\n",
        "\n",
        "    history_text = \"\\n\".join(conversation_history) if conversation_history else \"No previous conversation.\"\n",
        "\n",
        "    # Enhanced prompt with conversation history\n",
        "    prompt = f\"\"\"Based on the conversation history and context below, answer the current question.\n",
        "\n",
        "    Conversation History:\n",
        "    {history_text}\n",
        "\n",
        "    Context from documents:\n",
        "    {context}\n",
        "\n",
        "    Current Question: {current_query}\n",
        "\n",
        "    Answer (be specific and helpful):\"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    return {\n",
        "        \"response\": response.content,\n",
        "        \"messages\": messages + [AIMessage(content=response.content)],\n",
        "        \"source_documents\": relevant_docs\n",
        "    }\n",
        "\n",
        "\n",
        "# Routing Function\n",
        "def route_query(state: AgentState) -> Literal[\"direct_llm\", \"rag_pipeline\"]:\n",
        "    \"\"\"Route based on mode\"\"\"\n",
        "    if state[\"mode\"] == \"rag\":\n",
        "        return \"rag_pipeline\"\n",
        "    return \"direct_llm\"\n",
        "\n",
        "\n",
        "# Build Graph\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"direct_llm\", direct_llm_node)\n",
        "workflow.add_node(\"rag_pipeline\", rag_pipeline_node)\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_query,\n",
        "    {\n",
        "        \"direct_llm\": \"direct_llm\",\n",
        "        \"rag_pipeline\": \"rag_pipeline\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"direct_llm\", END)\n",
        "workflow.add_edge(\"rag_pipeline\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "\n",
        "#  CHAT FUNCTION\n",
        "conversation_history = []\n",
        "\n",
        "\n",
        "def chat(query: str, mode: str = \"rag\", pdf_directory: str = \"\"):\n",
        "    \"\"\"\n",
        "    Chat function that maintains conversation history\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        mode: \"rag\" or \"direct\"\n",
        "        pdf_directory: Path to directory containing PDFs (empty for default)\n",
        "\n",
        "    Returns:\n",
        "        dict with response and source_documents\n",
        "    \"\"\"\n",
        "    # Add user message to history\n",
        "    conversation_history.append(HumanMessage(content=query))\n",
        "\n",
        "    # Invoke with full conversation history\n",
        "    result = app.invoke({\n",
        "        \"messages\": conversation_history,\n",
        "        \"mode\": mode,\n",
        "        \"pdf_directory\": pdf_directory,\n",
        "        \"response\": \"\",\n",
        "        \"source_documents\": []\n",
        "    })\n",
        "\n",
        "    # Add AI response to history\n",
        "    conversation_history.append(AIMessage(content=result[\"response\"]))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def reset_conversation():\n",
        "    \"\"\"Clear conversation history to start fresh\"\"\"\n",
        "    global conversation_history\n",
        "    conversation_history = []\n",
        "    print(\" Conversation history cleared!\")\n",
        "\n",
        "\n",
        "def reset_faiss_index():\n",
        "    \"\"\"Delete FAISS index to force recreation from PDFs\"\"\"\n",
        "    import shutil\n",
        "    if os.path.exists(FAISS_INDEX_PATH):\n",
        "        shutil.rmtree(FAISS_INDEX_PATH)\n",
        "        print(f\" FAISS index deleted from {FAISS_INDEX_PATH}\")\n",
        "    else:\n",
        "        print(\" No FAISS index found to delete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34LfcVSKttx4",
        "outputId": "67da3b5c-ecdd-400e-88db-86904c738529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1: Direct LLM Mode\n",
            "\n",
            " Response: Of course! Let's cover that. We discussed it at the beginning of our conversation, but it's a foundational topic worth revisiting.\n",
            "\n",
            "### The Simple Analogy\n",
            "\n",
            "Imagine you're teaching a child about dogs.\n",
            "\n",
            "*   **Traditional AI:** You show them 1,000 photos and ask, \"Is this a dog?\" for each one. Eventually, they learn to *recognize* a dog.\n",
            "*   **Generative AI:** You show them 1,000 photos of dogs. After studying them all, you give them a blank piece of paper and say, \"Now, *draw a new dog* that has never existed before.\"\n",
            "\n",
            "**Generative AI** is the system that can draw that new dog. It learns the underlying patterns, features, and \"essence\" of the data it's trained on, and then uses that knowledge to **create new, original content**.\n",
            "\n",
            "---\n",
            "\n",
            "### The Formal Definition\n",
            "\n",
            "**Generative AI** is a branch of artificial intelligence that can generate novel content, including text, images, audio, code, and synthetic data. Unlike traditional AI models that are designed to classify or predict (i.e., *discriminate* between different inputs), generative models are designed to *create*.\n",
            "\n",
            "The key difference is **Creation vs. Recognition**.\n",
            "\n",
            "| **Traditional AI (Discriminative)** | **Generative AI** |\n",
            "| :--- | :--- |\n",
            "| **Goal:** To classify, predict, or identify patterns. | **Goal:** To create new, original content. |\n",
            "| **Question it answers:** \"Is this email spam or not spam?\" | **Question it answers:** \"Write me an email about a product launch.\" |\n",
            "| **Example:** A facial recognition system, a credit score predictor. | **Example:** ChatGPT, Midjourney, DALL-E. |\n",
            "\n",
            "---\n",
            "\n",
            "### How Does It Work?\n",
            "\n",
            "Generative AI models are built on massive neural networks, often called **Large Language Models (LLMs)** for text or **Diffusion Models** for images. The process is generally two-fold:\n",
            "\n",
            "1.  **Training:** The model is trained on an enormous dataset.\n",
            "    *   A text model like ChatGPT is fed a huge portion of the internet (books, articles, websites) to learn grammar, facts, reasoning, and different writing styles.\n",
            "    *   An image model like Midjourney is trained on billions of images paired with text descriptions, learning the connection between words (like \"dragon\") and visual concepts (what a dragon looks like).\n",
            "\n",
            "2.  **Generation (Inference):** When you give the model a **prompt** (an instruction), it uses its training to generate a response.\n",
            "    *   For text, it predicts the most likely next word, then the next, and so on, to build sentences and paragraphs that are statistically probable and coherent.\n",
            "    *   For images, it often starts with a pattern of random noise and gradually refines it, step-by-step, to match the text description in the prompt.\n",
            "\n",
            "---\n",
            "\n",
            "### What Can It Create?\n",
            "\n",
            "*   **Text:** Essays, emails, poems, marketing copy, computer code. (e.g., **ChatGPT, Google Gemini**)\n",
            "*   **Images:** Photorealistic images, paintings, logos, and art from text descriptions. (e.g., **DALL-E 3, Midjourney, Stable Diffusion**)\n",
            "*   **Audio:** Original music, realistic voiceovers, and sound effects. (e.g., **Suno, ElevenLabs**)\n",
            "*   **Video:** Short video clips generated from text prompts. (e.g., **OpenAI's Sora, RunwayML**)\n",
            "*   **Data:** Synthetic data for training other AI models, which is useful when real-world data is scarce or private.\n",
            "\n",
            "### Why Is It a Big Deal?\n",
            "\n",
            "Generative AI is considered a transformative technology because it democratizes creation, automates complex tasks, and acts as a powerful tool for brainstorming and productivity, impacting nearly every industry from software development to entertainment.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 1 - Direct LLM\n",
        "\n",
        "print(\"Example 1: Direct LLM Mode\")\n",
        "\n",
        "\n",
        "result0 = chat(\"What is Generative AI\", mode=\"direct\")\n",
        "\n",
        "print(\"\\n Response: \", end=\"\")\n",
        "for char in result0[\"response\"]:\n",
        "    print(char, end=\"\", flush=True)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "yzT0NH4wtumL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e7c8fa-d5b7-4866-e415-41735adcf684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 2: Direct LLM Follow-up\n",
            "\n",
            " Response: Of course. This is a crucial distinction that highlights why Generative AI is such a significant leap forward. We've touched on this before, and it's a great concept to solidify.\n",
            "\n",
            "The fundamental difference lies in their **primary purpose and output**.\n",
            "\n",
            "*   **Traditional Machine Learning** is primarily **discriminative**. Its goal is to **analyze existing data to make a prediction or classification**. It learns to *distinguish* between different categories.\n",
            "*   **Generative AI** is, as its name implies, **generative**. Its goal is to **learn from existing data to create new, original data** that has similar characteristics.\n",
            "\n",
            "Let's use a simple analogy:\n",
            "\n",
            "*   **Traditional ML is like a food critic.** A critic tastes a dish (input data) and gives a verdict (output), such as \"This is a high-quality pasta\" (classification) or \"I rate this 8/10\" (prediction). The critic can judge, but they can't create a new dish from scratch.\n",
            "*   **Generative AI is like a chef.** A chef studies hundreds of pasta recipes (training data) to understand the ingredients, textures, and flavor combinations. Then, they can use that knowledge to **create a completely new pasta dish** that has never existed before.\n",
            "\n",
            "---\n",
            "\n",
            "### Key Differences: Traditional Machine Learning vs. Generative AI\n",
            "\n",
            "Here is a side-by-side comparison to make the differences clear:\n",
            "\n",
            "| Feature | Traditional Machine Learning (Discriminative) | Generative AI |\n",
            "| :--- | :--- | :--- |\n",
            "| **Primary Goal** | **Predict or Classify.** | **Create or Generate.** |\n",
            "| **The Question it Answers** | \"Is this A or B?\"<br>\"How much will this cost?\"<br>\"Which group does this belong to?\" | \"Create a new example of A.\"<br>\"Write a story about B.\"<br>\"Generate an image of C.\" |\n",
            "| **Nature of the Output** | A specific, often simple, output.<br>(e.g., a class label like `Spam`, a number like `$350,000`) | Complex, unstructured, and original content.<br>(e.g., a paragraph of text, a high-resolution image, a piece of music) |\n",
            "| **How it Learns (The Core Logic)** | It learns the **decision boundary** that separates different data types. It focuses on the *differences* between categories. | It learns the **underlying probability distribution** of the data itself. It learns the *essence* of what makes the data what it is. |\n",
            "| **Common Tasks** | • **Classification:** Spam filtering, image recognition.<br>• **Regression:** Predicting stock prices, house values.<br>• **Clustering:** Segmenting customers. | • **Content Creation:** Writing articles, generating art.<br>• **Summarization & Translation.**<br>• **Data Augmentation:** Creating synthetic data to train other models. |\n",
            "| **Examples** | • Your email's spam filter.<br>• Netflix's movie recommendation engine.<br>• A bank's model for predicting loan defaults. | • **ChatGPT** (generating text).<br>• **Midjourney / DALL-E** (generating images).<br>• **GitHub Copilot** (generating code). |\n",
            "\n",
            "---\n",
            "\n",
            "### A Deeper Look at \"How It Learns\"\n",
            "\n",
            "This is the most important technical difference.\n",
            "\n",
            "#### 1. Traditional ML: Learning the Boundary\n",
            "\n",
            "Imagine you have data points for apples and oranges on a graph based on their color and shape. A traditional (discriminative) model learns one thing: **how to draw a line that best separates the apples from the oranges.**\n",
            "\n",
            "It doesn't need to understand what an apple truly *is*. It only needs to know that if a fruit is reddish and round, it's probably on the \"apple\" side of the line. You can't ask this model to \"create an apple\" because it has no concept of \"apple-ness,\" only the boundary that separates it from an orange.\n",
            "\n",
            "#### 2. Generative AI: Learning the Pattern\n",
            "\n",
            "A generative model looks at all the apple data points and learns the fundamental patterns—the distribution of their colors, shapes, sizes, and textures. It builds an internal, statistical representation of an \"ideal\" apple.\n",
            "\n",
            "Because it has learned this deep pattern, you can ask it to **generate a new point that fits this pattern**. It can create a new, unique image of an apple that is statistically consistent with all the apples it has ever seen.\n",
            "\n",
            "### Summary\n",
            "\n",
            "| | **Traditional Machine Learning** | **Generative AI** |\n",
            "|---|---|---|\n",
            "| **Focus** | Making **judgments about** existing data. | **Creating new instances of** data. |\n",
            "| **Analogy** | The Critic / The Judge | The Artist / The Creator |\n",
            "\n",
            "In essence, traditional machine learning is about analysis and prediction, while **Generative AI is about synthesis and creation**, marking a profound shift in the capabilities of artificial intelligence.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 2 - Direct LLM - follow-up question\n",
        "\n",
        "print(\"Example 2: Direct LLM Follow-up\")\n",
        "\n",
        "\n",
        "result00 = chat(\"How it was different from traditional machine learning?\", mode=\"direct\")\n",
        "\n",
        "print(\"\\n Response: \", end=\"\")\n",
        "for char in result00[\"response\"]:\n",
        "    print(char, end=\"\", flush=True)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "QXneUNx7tzUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8188feef-b28c-4c64-8b6d-d0dd648b4cdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 3: RAG Mode (Multiple PDFs)\n",
            "Loading existing FAISS index...\n",
            "\n",
            " Response: Based on the provided context, here is a specific and helpful explanation of Low-Rank Adaptation.\n",
            "\n",
            "**Low-Rank Adaptation (LoRA)** is a technique designed to efficiently fine-tune Large Language Models (LLMs). Its primary goal is to adapt a pre-trained model to a new task while significantly reducing memory usage and computational cost.\n",
            "\n",
            "According to the document, LoRA is a **parameter-efficient fine-tuning (PEFT)** method. Here’s how it works:\n",
            "\n",
            "1.  **Freezes the Original Model:** Instead of retraining all the billions of parameters in a Large Language Model (which is incredibly memory-intensive), LoRA freezes the original model.\n",
            "\n",
            "2.  **Adds Small, Trainable Layers:** It then introduces a small number of new, trainable parameters by adding **\"low-rank matrix adaptations\"** to the model's existing layers. These new layers are much smaller than the original ones.\n",
            "\n",
            "3.  **Efficient Training:** Only these small, new layers are updated during the fine-tuning process. This allows the model's behavior to be modified for a specific task without changing the original model's size or requiring the massive resources needed for a full fine-tune.\n",
            "\n",
            "### Key Benefits Mentioned in the Context:\n",
            "\n",
            "*   **Reduces Memory Usage:** By not training the full model, it dramatically lowers the memory overhead associated with fine-tuning.\n",
            "*   **Maintains Performance:** It achieves significant performance improvements on new tasks without compromising the model's original capabilities.\n",
            "*   **Reduced Data Needs:** It can perform well with fewer task-specific training examples.\n",
            "*   **Increased Flexibility:** The model can be easily and quickly adapted to various tasks using different small, trained LoRA adapters.\n",
            "\n",
            "--- Source Documents ---\n",
            "\n",
            " Source 1:\n",
            "   Page: 2\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: Q2. What is LoRA and QLoRA?\n",
            "Ans - LoRA and QLoRA are techniques designed to optimize the fine-\n",
            "tuning of Large Language Models (LLMs), focusing on reducing\n",
            "memory usage and enhancing efficiency without compromising\n",
            "performance in Natural Language Pro...\n",
            "\n",
            " Source 2:\n",
            "   Page: 2\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: new trainable parameters to modify a model's behavior without\n",
            "increasing its overall size. By doing so, LoRA maintains the original\n",
            "parameter count, reducing the memory overhead typically associated\n",
            "with training large models. It works by adding low-...\n",
            "\n",
            " Source 3:\n",
            "   Page: 44\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: Reduced Data Needs: It requires fewer examples to perform well,\n",
            "minimizing the need for large, task-specific datasets.\n",
            "Increased Flexibility: The model can easily adapt to various tasks\n",
            "with minimal additional training.\n",
            "Bhavishya Pandit...\n",
            "\n",
            " Source 4:\n",
            "   Page: 33\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: processing in a single model, improving parameter sharing and\n",
            "reducing complexity.\n",
            "2.Cross-Modality Attention: Enhanced interactions between text and\n",
            "images lead to better learning and stability during training.\n",
            "3.Data-Efficient Pretraining: Self-sup...\n",
            "\n",
            " Source 5:\n",
            "   Page: 15\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: parallel, speeding up both training and inference.\n",
            "Long-Range Dependencies: Seq2Seq models struggle with long-\n",
            "range dependencies. Transformers capture these effectively with\n",
            "self-attention, allowing the model to focus on any part of the\n",
            "sequence, re...\n"
          ]
        }
      ],
      "source": [
        "# Example 3: First RAG query\n",
        "print(\"Example 3: RAG Mode (Multiple PDFs)\")\n",
        "\n",
        "result1 = chat(\"What is Low rank adaptation?\", mode=\"rag\")\n",
        "\n",
        "print(\"\\n Response: \", end=\"\")\n",
        "for char in result1[\"response\"]:\n",
        "    print(char, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\\n--- Source Documents ---\")\n",
        "for idx, doc in enumerate(result1[\"source_documents\"], 1):\n",
        "    print(f\"\\n Source {idx}:\")\n",
        "    print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n",
        "    print(f\"   File: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print(f\"   Content: {doc.page_content[:250]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pzrf1K6gt1zf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a14396-9ad2-4150-99cb-ce87cae99863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 4: Follow-up Question\n",
            "Loading existing FAISS index...\n",
            "\n",
            " Response: Excellent question. Based on the provided context, the key difference is that **QLoRA is an enhanced version of LoRA that adds quantization to save even more memory.**\n",
            "\n",
            "Think of it this way:\n",
            "\n",
            "*   **LoRA** is a clever technique to avoid training the whole model.\n",
            "*   **QLoRA** takes that clever technique and adds another layer of optimization on top of it.\n",
            "\n",
            "Here is a more detailed breakdown based on the documents:\n",
            "\n",
            "### The Core Difference: Adding Quantization\n",
            "\n",
            "1.  **LoRA (Low-Rank Adaptation):**\n",
            "    *   **What it does:** It freezes the massive pre-trained model and adds small, trainable \"adapter\" layers.\n",
            "    *   **Its Goal:** To fine-tune the model for a new task without the huge memory cost of training all the original parameters. It focuses on making the *training process* more efficient.\n",
            "\n",
            "2.  **QLoRA (Quantized Low-Rank Adaptation):**\n",
            "    *   **What it does:** It does everything LoRA does, **PLUS** it compresses the large, frozen part of the model through **quantization**.\n",
            "    *   **How it Quantizes:** As the context states, it reduces the precision of the model's weights (e.g., from 16-bit to 4-bit), which is like storing the numbers in a more compact format.\n",
            "    *   **Its Goal:** To drastically reduce the *in-memory size* of the base model itself, allowing you to fine-tune even larger models on less hardware.\n",
            "\n",
            "### Comparison Table\n",
            "\n",
            "| Feature | LoRA (Low-Rank Adaptation) | QLoRA (Quantized LoRA) |\n",
            "| :--- | :--- | :--- |\n",
            "| **Core Idea** | Freeze the base model and train small, new \"adapter\" layers. | Do the same as LoRA, but also **quantize** the frozen base model. |\n",
            "| **Main Optimization** | Reduces the number of **trainable parameters**. | Reduces trainable parameters **AND** reduces the memory footprint of the **non-trainable parameters**. |\n",
            "| **Key Technique** | Adds low-rank matrices. | Adds low-rank matrices **AND** applies quantization (e.g., 4-bit Normal Float). |\n",
            "| **Primary Benefit** | Makes the *fine-tuning process* much more memory-efficient. | Makes the *entire model* significantly smaller in memory, enabling fine-tuning on consumer-grade hardware. |\n",
            "\n",
            "In summary, the context shows that **LoRA** makes fine-tuning more efficient by reducing what you need to train. **QLoRA** builds directly on that by also shrinking the part of the model you *aren't* training, resulting in a minimal memory footprint overall.\n",
            "\n",
            "\n",
            "--- Source Documents ---\n",
            "\n",
            " Source 1:\n",
            "   Page: 3\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: This makes it ideal for environments where computational resources\n",
            "are limited, yet high model accuracy is still required.\n",
            "QLoRA (Quantized LoRA)\n",
            "QLoRA builds on LoRA by incorporating quantization to further\n",
            "optimize memory usage. It uses techniques ...\n",
            "\n",
            " Source 2:\n",
            "   Page: 2\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: Q2. What is LoRA and QLoRA?\n",
            "Ans - LoRA and QLoRA are techniques designed to optimize the fine-\n",
            "tuning of Large Language Models (LLMs), focusing on reducing\n",
            "memory usage and enhancing efficiency without compromising\n",
            "performance in Natural Language Pro...\n",
            "\n",
            " Source 3:\n",
            "   Page: 2\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: new trainable parameters to modify a model's behavior without\n",
            "increasing its overall size. By doing so, LoRA maintains the original\n",
            "parameter count, reducing the memory overhead typically associated\n",
            "with training large models. It works by adding low-...\n",
            "\n",
            " Source 4:\n",
            "   Page: 3\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: model's parameters and improve computational efficiency. By\n",
            "reducing the precision of model weights (e.g., from 16-bit to 4-bit)\n",
            "while retaining most of the model's accuracy, QLoRA allows for the\n",
            "fine-tuning of LLMs with minimal memory footprint. Thi...\n",
            "\n",
            " Source 5:\n",
            "   Page: 16\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: Positional Encoding: Since Transformers process the entire\n",
            "sequence at once, positional encoding is used to ensure the\n",
            "model understands token order.\n",
            "Efficiency and Scalability: Seq2Seq models are slower to scale\n",
            "due to sequential processing. Transfo...\n"
          ]
        }
      ],
      "source": [
        "# Example 4: Follow-up question (uses conversation history!)\n",
        "\n",
        "print(\"Example 4: Follow-up Question\")\n",
        "\n",
        "\n",
        "result2 = chat(\"How it was different from Quantized LORA?\", mode=\"rag\")\n",
        "\n",
        "print(\"\\n Response: \", end=\"\")\n",
        "for char in result2[\"response\"]:\n",
        "    print(char, end=\"\", flush=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"\\n--- Source Documents ---\")\n",
        "for idx, doc in enumerate(result2[\"source_documents\"], 1):\n",
        "    print(f\"\\n Source {idx}:\")\n",
        "    print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n",
        "    print(f\"   File: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print(f\"   Content: {doc.page_content[:250]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Follow-up question (uses conversation history!)\n",
        "\n",
        "print(\"Example 5: Follow-up Question\")\n",
        "\n",
        "\n",
        "result3 = chat(\"What was the last question I asked you?\", mode=\"direct\")\n",
        "\n",
        "print(\"\\n Response: \", end=\"\")\n",
        "for char in result3[\"response\"]:\n",
        "    print(char, end=\"\", flush=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"\\n--- Source Documents ---\")\n",
        "for idx, doc in enumerate(result3[\"source_documents\"], 1):\n",
        "    print(f\"\\n Source {idx}:\")\n",
        "    print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n",
        "    print(f\"   File: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print(f\"   Content: {doc.page_content[:250]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGwQm0uZmqCK",
        "outputId": "99b568a6-3aca-4244-c7f0-1a92028c8863"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 5: Follow-up Question\n",
            "\n",
            " Response: The last question you asked me was:\n",
            "\n",
            "**\"How it was different from Quantized LORA?\"**\n",
            "\n",
            "\n",
            "--- Source Documents ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EevxkQ0Sm8q5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}