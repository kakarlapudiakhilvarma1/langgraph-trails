{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZNj8qWtslYp"
      },
      "source": [
        "#### **Agentic RAG application using LangGraph**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP6b71K5r66d",
        "outputId": "e7e51a13-5fe9-49d1-ebcc-7988d5098b6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: langchain_huggingface in /usr/local/lib/python3.12/dist-packages (1.0.0)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.12/dist-packages (1.0.2)\n",
            "Requirement already satisfied: langchain_google_genai in /usr/local/lib/python3.12/dist-packages (3.0.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.12/dist-packages (6.1.3)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (1.0.0)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.44)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (6.0.3)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (3.13.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.11.0)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.38)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.33.4 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.36.0)\n",
            "Requirement already satisfied: tokenizers<1.0.0,>=0.19.1 in /usr/local/lib/python3.12/dist-packages (from langchain_huggingface) (0.22.1)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (1.0.2)\n",
            "Requirement already satisfied: langgraph-sdk<0.3.0,>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from langgraph) (0.2.9)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph) (3.6.0)\n",
            "Requirement already satisfied: google-ai-generativelanguage<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (0.9.0)\n",
            "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain_google_genai) (1.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.22.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (2.28.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (2.38.0)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.76.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (5.29.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (2025.3.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0.0,>=0.33.4->langchain_huggingface) (1.2.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langchain-classic<2.0.0,>=1.0.0->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph) (1.11.0)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.3.0,>=0.2.2->langgraph) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain_community) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain_community) (1.2.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain_community) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain_community) (3.2.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.71.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (4.9.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (0.16.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain_google_genai) (0.6.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "pip install langchain_community langchain langchain_huggingface langchain-core langgraph langchain_google_genai pypdf faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKlGTZQqtM_w"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vqt8yyHFtTsH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import TypedDict, List, Literal\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "# Configuration\n",
        "DEFAULT_PDF_DIRECTORY = \"/content/data\"\n",
        "FAISS_INDEX_PATH = \"./faiss_index\"\n",
        "\n",
        "\n",
        "# Enhanced State Definition\n",
        "class AgentState(TypedDict):\n",
        "    messages: list\n",
        "    mode: str\n",
        "    pdf_directory: str\n",
        "    response: str\n",
        "    source_documents: List[Document]\n",
        "\n",
        "\n",
        "# Initialize LLM with streaming enabled\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-pro\",\n",
        "    temperature=0.3,\n",
        "    api_key=api_key,\n",
        "    #max_output_tokens= 1000,\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize embeddings globally\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "\n",
        "# Node 1: Direct LLM\n",
        "def direct_llm_node(state: AgentState):\n",
        "    \"\"\"Direct interaction with LLM with conversation history\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    return {\n",
        "        \"response\": response.content,\n",
        "        \"messages\": messages + [AIMessage(content=response.content)],\n",
        "        \"source_documents\": []\n",
        "    }\n",
        "\n",
        "\n",
        "# Node 2: RAG Pipeline with conversation history\n",
        "def rag_pipeline_node(state: AgentState):\n",
        "    \"\"\"Complete RAG pipeline with conversation history support\"\"\"\n",
        "    messages = state[\"messages\"]\n",
        "    current_query = messages[-1].content\n",
        "\n",
        "    pdf_directory = state.get(\"pdf_directory\", DEFAULT_PDF_DIRECTORY) or DEFAULT_PDF_DIRECTORY\n",
        "\n",
        "    # Load or create FAISS index\n",
        "    if os.path.exists(FAISS_INDEX_PATH):\n",
        "        print(\"Loading existing FAISS index...\")\n",
        "        vectorstore = FAISS.load_local(\n",
        "            FAISS_INDEX_PATH,\n",
        "            embeddings,\n",
        "            allow_dangerous_deserialization=True\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Creating new FAISS index from directory: {pdf_directory}\")\n",
        "\n",
        "        # Load all PDFs from directory\n",
        "        loader = PyPDFDirectoryLoader(\n",
        "            path=pdf_directory,\n",
        "            glob=\"**/*.pdf\",  # Load all PDFs recursively\n",
        "            recursive=True,    # Search subdirectories\n",
        "            silent_errors=False  # Show errors if any\n",
        "        )\n",
        "\n",
        "        documents = loader.load()\n",
        "        print(f\" Loaded {len(documents)} pages from PDF files in {pdf_directory}\")\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=500,\n",
        "            chunk_overlap=150\n",
        "        )\n",
        "        chunks = text_splitter.split_documents(documents)\n",
        "        print(f\" Created {len(chunks)} chunks from documents\")\n",
        "\n",
        "        vectorstore = FAISS.from_documents(chunks, embeddings)\n",
        "        vectorstore.save_local(FAISS_INDEX_PATH)\n",
        "        print(f\" FAISS index saved to {FAISS_INDEX_PATH}\")\n",
        "\n",
        "    # Retrieval\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "    relevant_docs = retriever.invoke(current_query)\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "\n",
        "    # Build conversation history for context\n",
        "    conversation_history = []\n",
        "    for msg in messages[:-1]:  # Exclude current query\n",
        "        if isinstance(msg, HumanMessage):\n",
        "            conversation_history.append(f\"User: {msg.content}\")\n",
        "        elif isinstance(msg, AIMessage):\n",
        "            conversation_history.append(f\"Assistant: {msg.content}\")\n",
        "\n",
        "    history_text = \"\\n\".join(conversation_history) if conversation_history else \"No previous conversation.\"\n",
        "\n",
        "    # Enhanced prompt with conversation history\n",
        "    prompt = f\"\"\"Based on the conversation history and context below, answer the current question.\n",
        "\n",
        "Conversation History:\n",
        "{history_text}\n",
        "\n",
        "Context from documents:\n",
        "{context}\n",
        "\n",
        "Current Question: {current_query}\n",
        "\n",
        "Answer (be specific and helpful):\"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "\n",
        "    return {\n",
        "        \"response\": response.content,\n",
        "        \"messages\": messages + [AIMessage(content=response.content)],\n",
        "        \"source_documents\": relevant_docs\n",
        "    }\n",
        "\n",
        "\n",
        "# Routing Function\n",
        "def route_query(state: AgentState) -> Literal[\"direct_llm\", \"rag_pipeline\"]:\n",
        "    \"\"\"Route based on mode\"\"\"\n",
        "    if state[\"mode\"] == \"rag\":\n",
        "        return \"rag_pipeline\"\n",
        "    return \"direct_llm\"\n",
        "\n",
        "\n",
        "# Build Graph\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"direct_llm\", direct_llm_node)\n",
        "workflow.add_node(\"rag_pipeline\", rag_pipeline_node)\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    START,\n",
        "    route_query,\n",
        "    {\n",
        "        \"direct_llm\": \"direct_llm\",\n",
        "        \"rag_pipeline\": \"rag_pipeline\"\n",
        "    }\n",
        ")\n",
        "\n",
        "workflow.add_edge(\"direct_llm\", END)\n",
        "workflow.add_edge(\"rag_pipeline\", END)\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "\n",
        "#  CHAT FUNCTION\n",
        "conversation_history = []\n",
        "\n",
        "\n",
        "def chat(query: str, mode: str = \"rag\", pdf_directory: str = \"\"):\n",
        "    \"\"\"\n",
        "    Chat function that maintains conversation history\n",
        "\n",
        "    Args:\n",
        "        query: User's question\n",
        "        mode: \"rag\" or \"direct\"\n",
        "        pdf_directory: Path to directory containing PDFs (empty for default)\n",
        "\n",
        "    Returns:\n",
        "        dict with response and source_documents\n",
        "    \"\"\"\n",
        "    # Add user message to history\n",
        "    conversation_history.append(HumanMessage(content=query))\n",
        "\n",
        "    # Invoke with full conversation history\n",
        "    result = app.invoke({\n",
        "        \"messages\": conversation_history,\n",
        "        \"mode\": mode,\n",
        "        \"pdf_directory\": pdf_directory,\n",
        "        \"response\": \"\",\n",
        "        \"source_documents\": []\n",
        "    })\n",
        "\n",
        "    # Add AI response to history\n",
        "    conversation_history.append(AIMessage(content=result[\"response\"]))\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def reset_conversation():\n",
        "    \"\"\"Clear conversation history to start fresh\"\"\"\n",
        "    global conversation_history\n",
        "    conversation_history = []\n",
        "    print(\" Conversation history cleared!\")\n",
        "\n",
        "\n",
        "def reset_faiss_index():\n",
        "    \"\"\"Delete FAISS index to force recreation from PDFs\"\"\"\n",
        "    import shutil\n",
        "    if os.path.exists(FAISS_INDEX_PATH):\n",
        "        shutil.rmtree(FAISS_INDEX_PATH)\n",
        "        print(f\" FAISS index deleted from {FAISS_INDEX_PATH}\")\n",
        "    else:\n",
        "        print(\" No FAISS index found to delete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34LfcVSKttx4",
        "outputId": "2ba8b623-51a1-43d8-881e-13477ec677a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Example 1: Direct LLM Mode\n",
            "==================================================\n",
            "\n",
            "ðŸ¤– Response: Of course! Here is a comprehensive explanation of Generative AI, broken down for easy understanding.\n",
            "\n",
            "### The Simple Analogy: The Creative Student\n",
            "\n",
            "Imagine two types of students studying for a history test.\n",
            "\n",
            "*   **The Memorizer (Traditional AI):** This student reads the textbook and memorizes all the facts and dates. If you ask, \"When was the Battle of Hastings?\" they can instantly answer \"1066.\" They are excellent at classifying and predicting based on the exact data they've learned.\n",
            "\n",
            "*   **The Creator (Generative AI):** This student also reads the textbook, but instead of just memorizing, they learn the *patterns*, the *causes and effects*, and the *storytelling style* of the historian. If you ask them to \"Write a short, dramatic paragraph from the perspective of a soldier at the Battle of Hastings,\" they can create something entirely new that is consistent with the style and facts they learned.\n",
            "\n",
            "**Generative AI is the creative student.** It doesn't just recognize patterns; it uses them to generate brand new, original content.\n",
            "\n",
            "---\n",
            "\n",
            "### Formal Definition\n",
            "\n",
            "**Generative AI** is a branch of artificial intelligence that can create novel contentâ€”such as text, images, music, code, and videoâ€”that has not existed before. It learns the underlying patterns and structures from vast amounts of training data and then uses that knowledge to produce new examples.\n",
            "\n",
            "The key word is **\"generate.\"** Unlike traditional AI that might classify data (Is this a cat or a dog?) or predict a value (What will the stock price be tomorrow?), Generative AI *creates*.\n",
            "\n",
            "---\n",
            "\n",
            "### How Does It Work? The Core Process\n",
            "\n",
            "While the underlying technology is incredibly complex, the process can be simplified into three main steps:\n",
            "\n",
            "1.  **Training:** A model is fed an enormous dataset.\n",
            "    *   For a text model like ChatGPT, this could be a huge portion of the internet (Wikipedia, books, articles).\n",
            "    *   For an image model like Midjourney, this would be billions of images with their text descriptions.\n",
            "    *   For a code model like GitHub Copilot, this would be vast repositories of public code.\n",
            "\n",
            "2.  **Learning Patterns:** During training, the model isn't memorizing the data. Instead, it's learning the statistical relationships, grammar, styles, concepts, and structures within the data. It learns what a \"dog\" looks like from thousands of angles, in different art styles, and in various contexts. It learns the rules of grammar, the flow of a conversation, and the structure of a Python function.\n",
            "\n",
            "3.  **Generation (Inference):** When you give it a **prompt** (a command or question), the model uses its learned patterns to generate a response. It does this by predicting the most likely next piece of the sequence.\n",
            "    *   For text, it predicts the next most likely word, then the next, and so on, forming coherent sentences.\n",
            "    *   For images, it starts with digital \"noise\" and gradually refines it into an image that matches the prompt, pixel by pixel.\n",
            "\n",
            "---\n",
            "\n",
            "### What Can Generative AI Do? (Examples)\n",
            "\n",
            "Generative AI is powering a revolution in many fields:\n",
            "\n",
            "*   **Text Generation:** Writing emails, articles, poems, and marketing copy. (e.g., **ChatGPT, Google Gemini**)\n",
            "*   **Image Generation:** Creating realistic photos, paintings, and illustrations from a text description. (e.g., **Midjourney, DALL-E 3, Stable Diffusion**)\n",
            "*   **Code Generation:** Writing, completing, and debugging computer code in various programming languages. (e.g., **GitHub Copilot**)\n",
            "*   **Audio & Music Generation:** Composing original music in different genres or creating realistic voiceovers. (e.g., **Suno AI, ElevenLabs**)\n",
            "*   **Video Generation:** Creating short video clips from text prompts. (e.g., **OpenAI's Sora, Runway ML**)\n",
            "*   **Data Synthesis:** Creating realistic but artificial data to train other AI models, especially in fields like medicine where real data is sensitive.\n",
            "\n",
            "---\n",
            "\n",
            "### Key Differences: Generative AI vs. Traditional AI\n",
            "\n",
            "| Feature | Traditional AI (Discriminative) | Generative AI |\n",
            "| :--- | :--- | :--- |\n",
            "| **Primary Goal** | To classify, predict, or identify patterns in existing data. | To create new, original data. |\n",
            "| **Output** | A label, a category, or a numerical prediction. | New content (text, image, audio, etc.). |\n",
            "| **Question it Answers** | \"Is this A or B?\" or \"What is the value of Y?\" | \"Create something new that looks like X.\" |\n",
            "| **Example** | An email spam filter (classifies as \"spam\" or \"not spam\"). | A tool that writes a new email for you. |\n",
            "\n",
            "---\n",
            "\n",
            "### Limitations and Challenges\n",
            "\n",
            "Generative AI is powerful, but it's not perfect. Key challenges include:\n",
            "\n",
            "*   **Hallucinations:** The AI can confidently \"make up\" facts, sources, or details that are completely wrong.\n",
            "*   **Bias:** If the training data contains societal biases (related to race, gender, etc.), the AI will learn and reproduce them.\n",
            "*   **Copyright and Ownership:** Who owns an AI-generated image? The user who wrote the prompt, the company that made the AI, or no one? These are ongoing legal questions.\n",
            "*   **Ethical Concerns:** The potential for misuse, such as creating deepfakes, generating misinformation, or automating jobs on a massive scale.\n",
            "*   **High Cost:** Training these massive models requires immense computational power, which is expensive and energy-intensive.\n",
            "\n",
            "In essence, **Generative AI is a powerful tool for creativity and automation.** It's a paradigm shift from AI that simply understands the world to AI that can add new things to it.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 1 - Direct LLM\n",
        "print(\"=\" * 50)\n",
        "print(\"Example 1: Direct LLM Mode\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "result0 = chat(\"What is Generative AI\", mode=\"direct\")\n",
        "\n",
        "print(\"\\nðŸ¤– Response: \", end=\"\")\n",
        "for char in result0[\"response\"]:\n",
        "    print(char, end=\"\", flush=True)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yzT0NH4wtumL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f377375-c335-4954-f480-5a5986e6fccd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Example 2: Direct LLM Follow-up\n",
            "==================================================\n",
            "\n",
            "ðŸ¤– Response: Excellent question. This gets to the heart of the recent AI revolution. The difference lies in their fundamental **goal and output**.\n",
            "\n",
            "Let's use a clear analogy to start.\n",
            "\n",
            "### The Analogy: The Art Critic vs. The Artist\n",
            "\n",
            "*   **Traditional Machine Learning is the Art Critic.** It can study thousands of paintings by Van Gogh. After its training, you can show it a new painting, and it can tell you with high accuracy, \"Yes, that is a Van Gogh\" or \"No, that is a forgery.\" It can *classify*, *predict*, and *identify*. **It judges existing work.**\n",
            "\n",
            "*   **Generative AI is the Artist.** It also studies thousands of paintings by Van Gogh. But instead of learning to judge, it learns the patterns, the brush strokes, the color palettes, and the essence of his style. Then, if you ask it to \"Paint a picture of a lighthouse in the style of Van Gogh,\" it can create a **brand new, original painting** that has never existed before but looks authentically like his work. **It creates new work.**\n",
            "\n",
            "This core differenceâ€”**judging vs. creating**â€”is the key.\n",
            "\n",
            "---\n",
            "\n",
            "### Detailed Comparison: Traditional ML vs. Generative AI\n",
            "\n",
            "Here is a table breaking down the key differences across several dimensions:\n",
            "\n",
            "| Feature | Traditional Machine Learning (Discriminative Models) | Generative AI (Generative Models) |\n",
            "| :--- | :--- | :--- |\n",
            "| **Primary Goal** | To **predict** an outcome or **classify** data based on learned patterns. | To **create** new, original data that resembles the data it was trained on. |\n",
            "| **Core Question** | It answers questions like: \"Is this A or B?\", \"How much?\", or \"What's next in this sequence?\" | It responds to commands like: \"Create an example of X,\" or \"Generate Y in the style of Z.\" |\n",
            "| **Output** | A simple, often numerical or categorical output (e.g., a number, a probability, a \"yes/no\", a class label like \"spam\"). | A complex, high-dimensional, and unstructured output (e.g., an entire image, a paragraph of text, a piece of music, lines of code). |\n",
            "| **How it \"Thinks\"** | It learns the **decision boundary** between different categories. It finds the line that best separates \"cats\" from \"dogs.\" | It learns the **underlying probability distribution** of the data. It learns what makes a \"cat\" a cat in all its possible forms, so it can generate a new one. |\n",
            "| **Example Task** | **Spam Detection:** Classifies an incoming email as \"spam\" or \"not spam.\" | **Email Composition:** Writes a brand new email for you based on a prompt like \"draft a polite follow-up.\" |\n",
            "| **Another Example** | **Image Recognition:** Identifies that a photo contains a \"dog.\" | **Image Generation:** Creates a completely new image of a \"dog wearing a superhero cape.\" |\n",
            "| **Data Needs** | Can often work with smaller, structured, and well-labeled datasets (e.g., a spreadsheet of customer data). | Requires massive, often unstructured datasets (e.g., the entire text of the internet, billions of images) to learn the deep patterns needed for creation. |\n",
            "\n",
            "---\n",
            "\n",
            "### When to Use Which?\n",
            "\n",
            "You would choose one over the other based on the problem you're trying to solve:\n",
            "\n",
            "#### Use Traditional Machine Learning when you need to:\n",
            "\n",
            "*   **Automate Decisions:** Is this credit card transaction fraudulent? (Classification)\n",
            "*   **Forecast the Future:** What will our sales be next quarter? (Regression)\n",
            "*   **Categorize Information:** Which customer segment does this user belong to? (Clustering)\n",
            "*   **Understand Data:** What are the key drivers of customer churn? (Analysis)\n",
            "\n",
            "In these cases, you are working with existing data to extract insights or make judgments.\n",
            "\n",
            "#### Use Generative AI when you need to:\n",
            "\n",
            "*   **Create Content:** Write a blog post, design a logo, or compose a jingle.\n",
            "*   **Augment Human Creativity:** Give a programmer code suggestions or help a writer overcome writer's block.\n",
            "*   **Synthesize Data:** Create realistic but artificial patient data for medical research without violating privacy.\n",
            "*   **Build Conversational Interfaces:** Power a chatbot that can have a natural, free-flowing conversation.\n",
            "\n",
            "In summary, the shift from traditional machine learning to Generative AI is a move from **analysis to synthesis**, from **understanding to creation**. While traditional ML is incredibly powerful for optimizing and understanding systems, Generative AI is unlocking new possibilities in creativity and content generation.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example 2 - Direct LLM - follow-up question\n",
        "print(\"=\" * 50)\n",
        "print(\"Example 2: Direct LLM Follow-up\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "result00 = chat(\"How it was different from traditional machine learning?\", mode=\"direct\")\n",
        "\n",
        "print(\"\\n Response: \", end=\"\")\n",
        "for char in result00[\"response\"]:\n",
        "    print(char, end=\"\", flush=True)\n",
        "print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QXneUNx7tzUd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f28118eb-833f-4094-afdb-73be9c61b942"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Example 3: RAG Mode (Multiple PDFs)\n",
            "==================================================\n",
            "Creating new FAISS index from directory: /content/data\n",
            " Loaded 56 pages from PDF files in /content/data\n",
            " Created 110 chunks from documents\n",
            " FAISS index saved to ./faiss_index\n",
            "\n",
            "ðŸ¤– Response: Of course. Based on the provided context and our previous conversations, here is a detailed explanation of Beam Search.\n",
            "\n",
            "We'll start with an analogy, just like before.\n",
            "\n",
            "### The Analogy: Navigating a Maze\n",
            "\n",
            "Imagine you're in a maze and want to find the exit as quickly as possible. You have two strategies:\n",
            "\n",
            "*   **Greedy Decoding (The Impulsive Explorer):** At every intersection, you instantly take the path that *looks* the most promising or points most directly toward the exit, without considering any other options. This is fast, but you might run into a dead end and have to backtrack, or you might miss a slightly longer but ultimately much faster route.\n",
            "\n",
            "*   **Beam Search (The Cautious Team of Explorers):** At every intersection, you don't just choose one path. Instead, you send a small team of, say, three explorers down the three most promising-looking paths. After they've walked for a minute, you check in. You abandon the paths that are looking worse and send your team of three down the *new* set of most promising paths from your current positions.\n",
            "\n",
            "**Beam Search is the cautious team of explorers.** It keeps a few of the best options open at each step, which prevents it from getting locked into a bad decision early on. This leads to a much better overall path (or in AI, a more coherent sentence).\n",
            "\n",
            "---\n",
            "\n",
            "### Formal Definition\n",
            "\n",
            "**Beam Search** is a text generation algorithm that explores multiple possible sequences of words at the same time to find the most likely overall sentence. Instead of just picking the single best word at each step, it keeps track of a fixed number (`k`) of the most probable partial sentences (called \"beams\") and expands them.\n",
            "\n",
            "Its main purpose is to improve the quality and coherence of generated text compared to simpler methods.\n",
            "\n",
            "---\n",
            "\n",
            "### How It Works: A Simple Example\n",
            "\n",
            "Let's say a language model is trying to complete the sentence \"The cat...\" and we set the **beam width (k) to 2**. This means we will always keep track of the top 2 most likely sequences.\n",
            "\n",
            "**Step 1: First Word**\n",
            "The model starts with \"The\". It calculates the probability for the next word.\n",
            "*   `cat` (probability: 0.4)\n",
            "*   `dog` (probability: 0.3)\n",
            "*   `car` (probability: 0.1)\n",
            "*   ...and so on.\n",
            "\n",
            "Since our beam width is 2, we keep the top 2 options:\n",
            "1.  \"The cat\"\n",
            "2.  \"The dog\"\n",
            "\n",
            "**Step 2: Second Word**\n",
            "Now, the model expands **both** beams to see what comes next.\n",
            "\n",
            "*   **From \"The cat\":**\n",
            "    *   `sat` (probability: 0.6) -> Full sequence: \"The cat sat\" (Total probability = 0.4 * 0.6 = **0.24**)\n",
            "    *   `ran` (probability: 0.2) -> Full sequence: \"The cat ran\" (Total probability = 0.4 * 0.2 = **0.08**)\n",
            "\n",
            "*   **From \"The dog\":**\n",
            "    *   `barked` (probability: 0.5) -> Full sequence: \"The dog barked\" (Total probability = 0.3 * 0.5 = **0.15**)\n",
            "    *   `slept` (probability: 0.4) -> Full sequence: \"The dog slept\" (Total probability = 0.3 * 0.4 = **0.12**)\n",
            "\n",
            "**Step 3: Pruning the Beams**\n",
            "We now have four candidate sequences with their total probabilities:\n",
            "1.  \"The cat sat\" (0.24)\n",
            "2.  \"The dog barked\" (0.15)\n",
            "3.  \"The dog slept\" (0.12)\n",
            "4.  \"The cat ran\" (0.08)\n",
            "\n",
            "Again, because our beam width is 2, we **discard the worst options** and keep only the top 2:\n",
            "1.  \"The cat sat\"\n",
            "2.  \"The dog barked\"\n",
            "\n",
            "The process repeats: the model will now try to find the next word for both \"The cat sat\" and \"The dog barked\", calculate the probabilities, and again keep only the top 2 overall sequences. This continues until the sentence is complete.\n",
            "\n",
            "---\n",
            "\n",
            "### Beam Search vs. Greedy Decoding\n",
            "\n",
            "This table summarizes the key differences, directly addressing the comparison in the provided document.\n",
            "\n",
            "| Feature | Greedy Decoding | Beam Search |\n",
            "| :--- | :--- | :--- |\n",
            "| **Strategy** | Picks the single highest-probability word at each step. | Explores multiple (`k`) parallel sequences at each step. |\n",
            "| **Number of Paths** | Follows only one path. | Maintains `k` paths (the \"beams\"). |\n",
            "| **Output Quality** | Faster, but can be repetitive, nonsensical, or get stuck in loops. Often lower quality. | Slower, but produces more coherent, contextually relevant, and higher-quality text. |\n",
            "| **Computational Cost** | Low. It's very fast and efficient. | Higher. It requires more memory and computation to track multiple options. |\n",
            "| **Risk** | High risk of making a mistake early on that ruins the rest of the sentence. | Lower risk, as it keeps better alternatives in consideration for longer. |\n",
            "\n",
            "In short, Beam Search is a crucial technique that balances computational cost and output quality. It is a significant improvement over Greedy Decoding and is a key reason why modern language models can generate such fluent and coherent long-form text.\n",
            "\n",
            "--- Source Documents ---\n",
            "\n",
            "ðŸ“„ Source 1:\n",
            "   Page: 4\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: Bhavishya Pandit\n",
            "Q3. What is beam search, and how does it differ from greedy\n",
            "decoding?\n",
            "Ans - Beam search is a search algorithm used during text generation\n",
            "to find the most likely sequence of words. Instead of choosing the\n",
            "single highest-probability w...\n",
            "\n",
            "ðŸ“„ Source 2:\n",
            "   Page: 4\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: maintaining a set of the top k candidates (beams). It balances\n",
            "between finding high-probability sequences and exploring\n",
            "alternative paths. This leads to more coherent and contextually\n",
            "appropriate outputs, especially in long-form text generation tasks...\n",
            "\n",
            "ðŸ“„ Source 3:\n",
            "   Page: 36\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: Q34. What are the key steps involved in the Retrieval-Augmented\n",
            "Generation (RAG) pipeline?\n",
            "Key steps in the Retrieval-Augmented Generation (RAG) pipeline are:\n",
            "1.Retrieval: The query is encoded and compared with precomputed\n",
            "document embeddings to retr...\n",
            "\n",
            "ðŸ“„ Source 4:\n",
            "   Page: 37\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: query, minimizing resource usage.\n",
            "Maintains high performance: The model dynamically selects the\n",
            "most relevant experts for each input, ensuring task complexity is\n",
            "handled effectively.\n",
            "MoE enables efficient scaling of LLMs, allowing larger models with\n",
            "...\n",
            "\n",
            "ðŸ“„ Source 5:\n",
            "   Page: 10\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: which is important for tasks like question answering, dialogue\n",
            "generation, and information retrieval.\n",
            "During pre-training, the model is fed two sentences:\n",
            "50% of the time, the second sentence is the actual next sentence in the document\n",
            "(positive pair...\n"
          ]
        }
      ],
      "source": [
        "# Example 3: First RAG query\n",
        "print(\"=\" * 50)\n",
        "print(\"Example 3: RAG Mode (Multiple PDFs)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "result1 = chat(\"What is Beam Search?\", mode=\"rag\")\n",
        "\n",
        "print(\"\\n Response: \", end=\"\")\n",
        "for char in result1[\"response\"]:\n",
        "    print(char, end=\"\", flush=True)\n",
        "\n",
        "print(\"\\n\\n--- Source Documents ---\")\n",
        "for idx, doc in enumerate(result1[\"source_documents\"], 1):\n",
        "    print(f\"\\nðŸ“„ Source {idx}:\")\n",
        "    print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n",
        "    print(f\"   File: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print(f\"   Content: {doc.page_content[:250]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "pzrf1K6gt1zf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab379c7b-ce64-462d-c5cd-5d2525755eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Example 4: Follow-up Question\n",
            "==================================================\n",
            "Loading existing FAISS index...\n",
            "\n",
            "ðŸ¤– Response: Based on the conversation history, the last question you asked me was:\n",
            "\n",
            "**\"What is Beam Search?\"**\n",
            "\n",
            "In response, I explained it using an analogy of a \"cautious team of explorers\" navigating a maze, compared it to Greedy Decoding, and provided a step-by-step example of how it works to generate more coherent text.\n",
            "\n",
            "\n",
            "--- Source Documents ---\n",
            "\n",
            "ðŸ“„ Source 1:\n",
            "   Page: 0\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: TOPTOPTOP50 LLM50 LLM50 LLM\n",
            "Bhavishya Pandit\n",
            "Interview QuestionsInterview Questions...\n",
            "\n",
            "ðŸ“„ Source 2:\n",
            "   Page: 38\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: Q36. What is Chain-of-Thought (CoT) prompting, and how does it\n",
            "improve complex reasoning in LLMs?\n",
            "-Chain-of-Thought (CoT) prompting helps LLMs handle complex\n",
            "reasoning by encouraging them to break down tasks into smaller,\n",
            "sequential steps. This impro...\n",
            "\n",
            "ðŸ“„ Source 3:\n",
            "   Page: 10\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: Bhavishya Pandit\n",
            "Q9. What is next sentence prediction and how is useful in language\n",
            "modelling?\n",
            "Ans - Next Sentence Prediction (NSP) is a key technique used in\n",
            "language modeling, particularly in training large models like BERT\n",
            "(Bidirectional Encoder R...\n",
            "\n",
            "ðŸ“„ Source 4:\n",
            "   Page: 36\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: relevance to the query.\n",
            "3. Generation: The top-ranked documents are used as context by the\n",
            "LLM to generate more informed and accurate responses.\n",
            "This hybrid approach enhances the modelâ€™s ability to produce\n",
            "context-aware outputs by incorporating exter...\n",
            "\n",
            "ðŸ“„ Source 5:\n",
            "   Page: 10\n",
            "   File: /content/data/50 LLM Interview Questions.pdf\n",
            "   Content: the correct next sentence or not. This binary classification task is used alongside a\n",
            "masked language modeling task to improve the model's overall language\n",
            "understanding....\n"
          ]
        }
      ],
      "source": [
        "# Example 4: Follow-up question (uses conversation history!)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Example 4: Follow-up Question\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "result2 = chat(\"What was the last question I asked you?\", mode=\"rag\")\n",
        "\n",
        "print(\"\\nðŸ¤– Response: \", end=\"\")\n",
        "for char in result2[\"response\"]:\n",
        "    print(char, end=\"\", flush=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"\\n--- Source Documents ---\")\n",
        "for idx, doc in enumerate(result2[\"source_documents\"], 1):\n",
        "    print(f\"\\nðŸ“„ Source {idx}:\")\n",
        "    print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n",
        "    print(f\"   File: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print(f\"   Content: {doc.page_content[:250]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 5: Follow-up question (uses conversation history!)\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"Example 5: Follow-up Question\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "result3 = chat(\"What was the last question I asked you?\", mode=\"direct\")\n",
        "\n",
        "print(\"\\nðŸ¤– Response: \", end=\"\")\n",
        "for char in result3[\"response\"]:\n",
        "    print(char, end=\"\", flush=True)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"\\n--- Source Documents ---\")\n",
        "for idx, doc in enumerate(result3[\"source_documents\"], 1):\n",
        "    print(f\"\\nðŸ“„ Source {idx}:\")\n",
        "    print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n",
        "    print(f\"   File: {doc.metadata.get('source', 'N/A')}\")\n",
        "    print(f\"   Content: {doc.page_content[:250]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGwQm0uZmqCK",
        "outputId": "c2f388e5-45b1-4032-fdb2-c67900ef2b38"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Example 5: Follow-up Question\n",
            "==================================================\n",
            "\n",
            "ðŸ¤– Response: The last question you asked me was:\n",
            "\n",
            "**\"What was the last question I asked you?\"**\n",
            "\n",
            "Before that, you asked me about \"What is Beam Search?\".\n",
            "\n",
            "\n",
            "--- Source Documents ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EevxkQ0Sm8q5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}