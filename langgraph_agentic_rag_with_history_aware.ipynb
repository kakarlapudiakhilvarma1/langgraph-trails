{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMV2O8Wd53+Xri5aOTBW1f/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### **langgraph_agentic_rag_with_history_aware**"],"metadata":{"id":"RhIvXsO3Y_8Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fY54EiwDXqTg"},"outputs":[],"source":["pip install langchain_community langchain langchain_huggingface langchain-core langgraph langchain_google_genai pypdf faiss-cpu"]},{"cell_type":"code","source":["import os\n","from typing import TypedDict, List, Literal\n","from langchain_core.messages import HumanMessage, AIMessage\n","from langgraph.graph import StateGraph, START, END\n","from langchain_google_genai import ChatGoogleGenerativeAI\n","from langchain_community.document_loaders import PyPDFLoader\n","from langchain_text_splitters import RecursiveCharacterTextSplitter\n","from langchain_huggingface import HuggingFaceEmbeddings\n","from langchain_community.vectorstores import FAISS\n","from langchain_core.documents import Document\n","\n","# Configuration\n","DEFAULT_PDF_PATH = \"/content/50 LLM Interview Questions.pdf\"\n","FAISS_INDEX_PATH = \"./faiss_index\"\n","\n","# Enhanced State Definition\n","class AgentState(TypedDict):\n","    messages: list\n","    mode: str\n","    pdf_path: str\n","    response: str\n","    source_documents: List[Document]\n","\n","# Initialize LLM with streaming enabled\n","llm = ChatGoogleGenerativeAI(\n","    model=\"gemini-2.5-pro\",\n","    temperature=0.3,\n","    api_key=api_key,\n","    streaming=True\n",")\n","\n","# Initialize embeddings globally\n","embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","\n","# Node 1: Direct LLM\n","def direct_llm_node(state: AgentState):\n","    \"\"\"Direct interaction with LLM with conversation history\"\"\"\n","    messages = state[\"messages\"]\n","    response = llm.invoke(messages)\n","\n","    return {\n","        \"response\": response.content,\n","        \"messages\": messages + [AIMessage(content=response.content)],\n","        \"source_documents\": []\n","    }\n","\n","# Node 2: RAG Pipeline with conversation history\n","def rag_pipeline_node(state: AgentState):\n","    \"\"\"Complete RAG pipeline with conversation history support\"\"\"\n","    messages = state[\"messages\"]\n","    current_query = messages[-1].content\n","\n","    pdf_path = state.get(\"pdf_path\", DEFAULT_PDF_PATH) or DEFAULT_PDF_PATH\n","\n","    # Load or create FAISS index\n","    if os.path.exists(FAISS_INDEX_PATH):\n","        print(\"Loading existing FAISS index...\")\n","        vectorstore = FAISS.load_local(\n","            FAISS_INDEX_PATH,\n","            embeddings,\n","            allow_dangerous_deserialization=True\n","        )\n","    else:\n","        print(\"Creating new FAISS index...\")\n","        loader = PyPDFLoader(pdf_path)\n","        documents = loader.load()\n","\n","        text_splitter = RecursiveCharacterTextSplitter(\n","            chunk_size=500,\n","            chunk_overlap=150\n","        )\n","        chunks = text_splitter.split_documents(documents)\n","        vectorstore = FAISS.from_documents(chunks, embeddings)\n","        vectorstore.save_local(FAISS_INDEX_PATH)\n","        print(f\"FAISS index saved to {FAISS_INDEX_PATH}\")\n","\n","    # Retrieval\n","    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n","    relevant_docs = retriever.invoke(current_query)\n","    context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n","\n","    # Build conversation history for context\n","    conversation_history = []\n","    for msg in messages[:-1]:  # Exclude current query\n","        if isinstance(msg, HumanMessage):\n","            conversation_history.append(f\"User: {msg.content}\")\n","        elif isinstance(msg, AIMessage):\n","            conversation_history.append(f\"Assistant: {msg.content}\")\n","\n","    history_text = \"\\n\".join(conversation_history) if conversation_history else \"No previous conversation.\"\n","\n","    # Enhanced prompt with conversation history\n","    prompt = f\"\"\"Based on the conversation history and context below, answer the current question.\n","\n","Conversation History:\n","{history_text}\n","\n","Context from documents:\n","{context}\n","\n","Current Question: {current_query}\n","\n","Answer (be specific and helpful):\"\"\"\n","\n","    response = llm.invoke(prompt)\n","\n","    return {\n","        \"response\": response.content,\n","        \"messages\": messages + [AIMessage(content=response.content)],\n","        \"source_documents\": relevant_docs\n","    }\n","\n","# Routing Function\n","def route_query(state: AgentState) -> Literal[\"direct_llm\", \"rag_pipeline\"]:\n","    \"\"\"Route based on mode\"\"\"\n","    if state[\"mode\"] == \"rag\":\n","        return \"rag_pipeline\"\n","    return \"direct_llm\"\n","\n","# Build Graph\n","workflow = StateGraph(AgentState)\n","workflow.add_node(\"direct_llm\", direct_llm_node)\n","workflow.add_node(\"rag_pipeline\", rag_pipeline_node)\n","\n","workflow.add_conditional_edges(\n","    START,\n","    route_query,\n","    {\n","        \"direct_llm\": \"direct_llm\",\n","        \"rag_pipeline\": \"rag_pipeline\"\n","    }\n",")\n","\n","workflow.add_edge(\"direct_llm\", END)\n","workflow.add_edge(\"rag_pipeline\", END)\n","\n","# Compile\n","app = workflow.compile()\n","\n","# ==================== CHAT FUNCTION ====================\n","# Maintain conversation history across multiple turns\n","conversation_history = []\n","\n","def chat(query: str, mode: str = \"rag\", pdf_path: str = \"\"):\n","    \"\"\"\n","    Chat function that maintains conversation history\n","\n","    Args:\n","        query: User's question\n","        mode: \"rag\" or \"direct\"\n","        pdf_path: Path to PDF (empty for default)\n","\n","    Returns:\n","        dict with response and source_documents\n","    \"\"\"\n","    # Add user message to history\n","    conversation_history.append(HumanMessage(content=query))\n","\n","    # Invoke with full conversation history\n","    result = app.invoke({\n","        \"messages\": conversation_history,\n","        \"mode\": mode,\n","        \"pdf_path\": pdf_path,\n","        \"response\": \"\",\n","        \"source_documents\": []\n","    })\n","\n","    # Add AI response to history\n","    conversation_history.append(AIMessage(content=result[\"response\"]))\n","\n","    return result\n","\n","def reset_conversation():\n","    \"\"\"Clear conversation history to start fresh\"\"\"\n","    global conversation_history\n","    conversation_history = []\n","    print(\" Conversation history cleared!\")"],"metadata":{"id":"4omeEpBaXuho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example 1 - Direct LLM\n","\n","result0 = chat(\"What is Generative AI\", mode=\"direct\")\n","\n","print(\"\\nResponse: \", end=\"\")\n","for char in result0[\"response\"]:\n","    print(char, end=\"\", flush=True)\n","print(\"\\n\")\n"],"metadata":{"id":"D5L0dp5DXzd1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example 2 - Direct LLM - follow-up question\n","\n","result00 = chat(\"How it was different from tradition machine learning?\", mode=\"direct\")\n","\n","print(\"\\nResponse: \", end=\"\")\n","for char in result00[\"response\"]:\n","    print(char, end=\"\", flush=True)\n","print(\"\\n\")"],"metadata":{"id":"rP4xbhiuX0dC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **RAG**"],"metadata":{"id":"tfz9TDIqX3Jg"}},{"cell_type":"code","source":["# Example 1: First RAG query\n","print(\"=\" * 50)\n","print(\"Example 1: RAG Mode (First query)\")\n","print(\"=\" * 50)\n","\n","result1 = chat(\"What is Beam Search?\", mode=\"rag\")\n","\n","print(\"\\nResponse: \", end=\"\")\n","for char in result1[\"response\"]:\n","    print(char, end=\"\", flush=True)\n","\n","print(\"\\n\\n--- Source Documents ---\")\n","for idx, doc in enumerate(result1[\"source_documents\"], 1):\n","    print(f\"\\n Source {idx}:\")\n","    print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n","    print(f\"   Content: {doc.page_content[:250]}...\")\n","    print(f\"   Source: {doc.metadata.get('source', 'N/A')}\")"],"metadata":{"id":"dXnJiErXX46k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example 2: Follow-up question (uses conversation history!)\n","print(\"\\n\" + \"=\" * 50)\n","print(\"Example 2: Follow-up Question\")\n","print(\"=\" * 50)\n","\n","result2 = chat(\"How it is different from other models?\", mode=\"rag\")\n","\n","print(\"\\nResponse: \", end=\"\")\n","for char in result2[\"response\"]:\n","    print(char, end=\"\", flush=True)\n","print(\"\\n\")\n","\n","print(\"\\n--- Source Documents ---\")\n","for idx, doc in enumerate(result2[\"source_documents\"], 1):\n","    print(f\"\\nðŸ“„ Source {idx}:\")\n","    print(f\"   Page: {doc.metadata.get('page', 'N/A')}\")\n","    print(f\"   Content: {doc.page_content[:250]}...\")\n","    print(f\"   Source: {doc.metadata.get('source', 'N/A')}\")\n"],"metadata":{"id":"g2GRsxoWYF8K"},"execution_count":null,"outputs":[]}]}